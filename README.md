# Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation

## ðŸ”¨ Setup environment

Please run the following commands to set up the environment:

```bash
conda env create RAGOrign python=3.12
conda activate RAGOrign 
pip install -r requirements.txt
```

## Getting Started

### 1. Set up OpenAI API Key

Ensure your OpenAI API key is set as an environment variable or directly in the `OpenAI_API.py` configuration.

```bash
export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
export OPENAI_API_URL="YOUR_OPENAI_BASE_URL" # Optional, if using a custom endpoint
```

### 2. Collecting misgeneration events

RAGOrigin expects misgeneration events in a specific JSON format, typically generated by an attack simulation. For each misgeneration event, it should contain information such as questions, contexts, RAG responses, and retrieval scores. An example is provided in the`attack_feedback/PRAGB/*.json`.

### 3. Running

You can run `RAGOrigin.py` directly with specific arguments:

```bash
python RAGOrigin.py \
    --dataset "NQ" \
    --attack_retriever "e5" \
    --attack_LLM "gpt-4o-mini" \
    --judge_LLM "gpt-4o-mini" \
    --attack_method "PRAGB" \
    --attack_M 5 \
    --top_K 5 \
    --trace_method "RAGOrigin" \
    --proxy_model "meta-llama/Llama-3.1-8B" \
    --variant 0 \
    --normalize_method "z_score_normalize" \
    --feedback_root_dir "attack_feedback" \
    --feedback_scope_dir "attack_feedback_scope" \
    --result_root_dir "result" \
    --test_version "v1" \
    --cuda_device 0 
```

### Citation

The citations for our attribution framework:

```bib
@inproceedings{zhang2026ragorigin,
title={Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation},
author={Zhang, Baolei and Xin, Haoran and Chen, Yuxi and Liu, Zhuqing and Yi, Biao and Li, Tong and Nie, Lihai and Liu, Zheli and Fang, Minghong},
booktitle={IEEE Symposium on Security and Privacy},
year={2026}
}
```

